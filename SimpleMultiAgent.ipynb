{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEYrzG2vB8Ip"
      },
      "outputs": [],
      "source": [
        "!!pip install litellm\n",
        "\n",
        "\n",
        "# Set Groq API credentials and model directly\n",
        "import litellm\n",
        "\n",
        "#api_key = \"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\"\n",
        "api_key = \"API KEY\"\n",
        "model = \"gorq/meta-llama/llama-4-scout-17b-16e-instruct\"\n",
        "base_url = \"https://api.groq.com/openai/v1\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mwe2eeOQB0cC"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import time\n",
        "import traceback\n",
        "import inspect\n",
        "from litellm import completion\n",
        "from dataclasses import dataclass, field\n",
        "from typing import get_type_hints, List, Callable, Dict, Any, Optional\n",
        "\n",
        "tools = {}\n",
        "tools_by_tag = {}\n",
        "\n",
        "\n",
        "def to_openai_tools(tools_metadata: List[dict]):\n",
        "    openai_tools = [\n",
        "        {\n",
        "            \"type\": \"function\",\n",
        "            \"function\": {\n",
        "                \"name\": t['tool_name'],\n",
        "                # Include up to 1024 characters of the description\n",
        "                \"description\": t.get('description',\"\")[:1024],\n",
        "                \"parameters\": t.get('parameters',{}),\n",
        "            },\n",
        "        } for t in tools_metadata\n",
        "    ]\n",
        "    return openai_tools\n",
        "\n",
        "def get_tool_metadata(func, tool_name=None, description=None, parameters_override=None, terminal=False, tags=None):\n",
        "    \"\"\"\n",
        "    Extracts metadata for a function to use in tool registration.\n",
        "\n",
        "    Parameters:\n",
        "        func (function): The function to extract metadata from.\n",
        "        tool_name (str, optional): The name of the tool. Defaults to the function name.\n",
        "        description (str, optional): Description of the tool. Defaults to the function's docstring.\n",
        "        parameters_override (dict, optional): Override for the argument schema. Defaults to dynamically inferred schema.\n",
        "        terminal (bool, optional): Whether the tool is terminal. Defaults to False.\n",
        "        tags (List[str], optional): List of tags to associate with the tool.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing metadata about the tool, including description, args schema, and the function.\n",
        "    \"\"\"\n",
        "    # Default tool_name to the function name if not provided\n",
        "    tool_name = tool_name or func.__name__\n",
        "\n",
        "    # Default description to the function's docstring if not provided\n",
        "    description = description or (func.__doc__.strip() if func.__doc__ else \"No description provided.\")\n",
        "\n",
        "    # Discover the function's signature and type hints if no args_override is provided\n",
        "    if parameters_override is None:\n",
        "        signature = inspect.signature(func)\n",
        "        type_hints = get_type_hints(func)\n",
        "\n",
        "        # Build the arguments schema dynamically\n",
        "        args_schema = {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {},\n",
        "            \"required\": []\n",
        "        }\n",
        "        for param_name, param in signature.parameters.items():\n",
        "\n",
        "            if param_name in [\"action_context\", \"action_agent\"]:\n",
        "                continue  # Skip these parameters\n",
        "\n",
        "            def get_json_type(param_type):\n",
        "                if param_type == str:\n",
        "                    return \"string\"\n",
        "                elif param_type == int:\n",
        "                    return \"integer\"\n",
        "                elif param_type == float:\n",
        "                    return \"number\"\n",
        "                elif param_type == bool:\n",
        "                    return \"boolean\"\n",
        "                elif param_type == list:\n",
        "                    return \"array\"\n",
        "                elif param_type == dict:\n",
        "                    return \"object\"\n",
        "                else:\n",
        "                    return \"string\"\n",
        "\n",
        "            # Add parameter details\n",
        "            param_type = type_hints.get(param_name, str)  # Default to string if type is not annotated\n",
        "            param_schema = {\"type\": get_json_type(param_type)}  # Convert Python types to JSON schema types\n",
        "\n",
        "            args_schema[\"properties\"][param_name] = param_schema\n",
        "\n",
        "            # Add to required if not defaulted\n",
        "            if param.default == inspect.Parameter.empty:\n",
        "                args_schema[\"required\"].append(param_name)\n",
        "    else:\n",
        "        args_schema = parameters_override\n",
        "\n",
        "    # Return the metadata as a dictionary\n",
        "    return {\n",
        "        \"tool_name\": tool_name,\n",
        "        \"description\": description,\n",
        "        \"parameters\": args_schema,\n",
        "        \"function\": func,\n",
        "        \"terminal\": terminal,\n",
        "        \"tags\": tags or []\n",
        "    }\n",
        "\n",
        "\n",
        "def register_tool(tool_name=None, description=None, parameters_override=None, terminal=False, tags=None):\n",
        "    \"\"\"\n",
        "    A decorator to dynamically register a function in the tools dictionary with its parameters, schema, and docstring.\n",
        "\n",
        "    Parameters:\n",
        "        tool_name (str, optional): The name of the tool to register. Defaults to the function name.\n",
        "        description (str, optional): Override for the tool's description. Defaults to the function's docstring.\n",
        "        parameters_override (dict, optional): Override for the argument schema. Defaults to dynamically inferred schema.\n",
        "        terminal (bool, optional): Whether the tool is terminal. Defaults to False.\n",
        "        tags (List[str], optional): List of tags to associate with the tool.\n",
        "\n",
        "    Returns:\n",
        "        function: The wrapped function.\n",
        "    \"\"\"\n",
        "    def decorator(func):\n",
        "        # Use the reusable function to extract metadata\n",
        "        metadata = get_tool_metadata(\n",
        "            func=func,\n",
        "            tool_name=tool_name,\n",
        "            description=description,\n",
        "            parameters_override=parameters_override,\n",
        "            terminal=terminal,\n",
        "            tags=tags\n",
        "        )\n",
        "\n",
        "        # Register the tool in the global dictionary\n",
        "        tools[metadata[\"tool_name\"]] = {\n",
        "            \"description\": metadata[\"description\"],\n",
        "            \"parameters\": metadata[\"parameters\"],\n",
        "            \"function\": metadata[\"function\"],\n",
        "            \"terminal\": metadata[\"terminal\"],\n",
        "            \"tags\": metadata[\"tags\"] or []\n",
        "        }\n",
        "\n",
        "        for tag in metadata[\"tags\"]:\n",
        "            if tag not in tools_by_tag:\n",
        "                tools_by_tag[tag] = []\n",
        "            tools_by_tag[tag].append(metadata[\"tool_name\"])\n",
        "\n",
        "        return func\n",
        "    return decorator\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Prompt:\n",
        "    messages: List[Dict] = field(default_factory=list)\n",
        "    tools: List[Dict] = field(default_factory=list)\n",
        "    metadata: dict = field(default_factory=dict)  # Fixing mutable default issue\n",
        "\n",
        "def generate_response(prompt: Prompt) -> str:\n",
        "    messages = prompt.messages\n",
        "    tools = prompt.tools\n",
        "\n",
        "    model = \"groq/meta-llama/llama-4-scout-17b-16e-instruct\"\n",
        "    base_url = \"https://api.groq.com/openai/v1\"\n",
        "    api_key = \"API KEY\"\n",
        "\n",
        "    if not tools:\n",
        "        response = completion(\n",
        "            model=model,\n",
        "            base_url=base_url,\n",
        "            api_key=api_key,\n",
        "            messages=messages,\n",
        "            max_tokens=1024,\n",
        "            temperature=0.0\n",
        "        )\n",
        "    else:\n",
        "        response = completion(\n",
        "            model=model,\n",
        "            base_url=base_url,\n",
        "            api_key=api_key,\n",
        "            messages=messages,\n",
        "            tools=tools,\n",
        "            tool_choice=\"auto\",\n",
        "            #function_call=\"auto\",\n",
        "            max_tokens=1500,\n",
        "            temperature=0.0,\n",
        "        )\n",
        "\n",
        "    print(\"=== Full API Response ===\")\n",
        "    print(response)\n",
        "    print(\"=========================\")\n",
        "\n",
        "    raw_output = response.choices[0].message.content\n",
        "\n",
        "    print(f\"Raw model output:\\n{raw_output}\\n--- End of output ---\\n\")\n",
        "\n",
        "    # Fallback if model output is None or empty string\n",
        "    if not raw_output or raw_output.strip() == \"\":\n",
        "        # Return fallback action as JSON string to prompt listing files again\n",
        "        fallback = json.dumps({\"tool\": \"list_project_files\", \"args\": {}})\n",
        "        print(\"[Warning] Model returned empty response. Using fallback:\", fallback)\n",
        "        return fallback\n",
        "\n",
        "    return raw_output\n",
        "\n",
        "\n",
        "\n",
        "# def generate_response(prompt: Prompt) -> str:\n",
        "#     \"\"\"Call LLM to get response\"\"\"\n",
        "\n",
        "#     messages = prompt.messages\n",
        "#     tools = prompt.tools\n",
        "\n",
        "#     result = None\n",
        "\n",
        "#     if not tools:\n",
        "#         response = completion(\n",
        "#             model=\"openai/gpt-4o\",\n",
        "#             messages=messages,\n",
        "#             max_tokens=1024\n",
        "#         )\n",
        "#         result = response.choices[0].message.content\n",
        "#     else:\n",
        "#         response = completion(\n",
        "#             model=\"openai/gpt-4o\",\n",
        "#             messages=messages,\n",
        "#             tools=tools,\n",
        "#             max_tokens=1024\n",
        "#         )\n",
        "\n",
        "#         if response.choices[0].message.tool_calls:\n",
        "#             tool = response.choices[0].message.tool_calls[0]\n",
        "#             result = {\n",
        "#                 \"tool\": tool.function.name,\n",
        "#                 \"args\": json.loads(tool.function.arguments),\n",
        "#             }\n",
        "#             result = json.dumps(result)\n",
        "#         else:\n",
        "#             result = response.choices[0].message.content\n",
        "\n",
        "\n",
        "#     return result\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Goal:\n",
        "    priority: int\n",
        "    name: str\n",
        "    description: str\n",
        "\n",
        "\n",
        "class Action:\n",
        "    def __init__(self,\n",
        "                 name: str,\n",
        "                 function: Callable,\n",
        "                 description: str,\n",
        "                 parameters: Dict,\n",
        "                 terminal: bool = False):\n",
        "        self.name = name\n",
        "        self.function = function\n",
        "        self.description = description\n",
        "        self.terminal = terminal\n",
        "        self.parameters = parameters\n",
        "\n",
        "    def execute(self, **args) -> Any:\n",
        "        \"\"\"Execute the action's function\"\"\"\n",
        "        return self.function(**args)\n",
        "\n",
        "\n",
        "class ActionRegistry:\n",
        "    def __init__(self):\n",
        "        self.actions = {}\n",
        "\n",
        "    def register(self, action: Action):\n",
        "        self.actions[action.name] = action\n",
        "\n",
        "    def get_action(self, name: str) -> Optional[Action]:\n",
        "        return self.actions.get(name, None)\n",
        "\n",
        "    # def get_action(self, response):\n",
        "    #   invocation = self.agent_language.parse_response(response)\n",
        "\n",
        "    #   if not isinstance(invocation, dict):\n",
        "    #       raise ValueError(f\"parse_response did not return a dict: got {invocation} (type: {type(invocation)})\")\n",
        "    #   if \"tool\" not in invocation:\n",
        "    #       raise ValueError(f\"'tool' key not found in invocation: {invocation}\")\n",
        "\n",
        "    #   action = self.actions.get_action(invocation[\"tool\"])\n",
        "    #   return action, invocation\n",
        "\n",
        "\n",
        "    def get_actions(self) -> List[Action]:\n",
        "        \"\"\"Get all registered actions\"\"\"\n",
        "        return list(self.actions.values())\n",
        "\n",
        "\n",
        "class Memory:\n",
        "    def __init__(self):\n",
        "        self.items = []  # Basic conversation histor\n",
        "\n",
        "    def add_memory(self, memory: dict):\n",
        "        \"\"\"Add memory to working memory\"\"\"\n",
        "        self.items.append(memory)\n",
        "\n",
        "    def get_memories(self, limit: int = None) -> List[Dict]:\n",
        "        \"\"\"Get formatted conversation history for prompt\"\"\"\n",
        "        return self.items[:limit]\n",
        "\n",
        "    def copy_without_system_memories(self):\n",
        "        \"\"\"Return a copy of the memory without system memories\"\"\"\n",
        "        filtered_items = [m for m in self.items if m[\"type\"] != \"system\"]\n",
        "        memory = Memory()\n",
        "        memory.items = filtered_items\n",
        "        return memory\n",
        "\n",
        "\n",
        "class Environment:\n",
        "    def execute_action(self, action: Action, args: dict) -> dict:\n",
        "        \"\"\"Execute an action and return the result.\"\"\"\n",
        "        try:\n",
        "            result = action.execute(**args)\n",
        "            return self.format_result(result)\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"tool_executed\": False,\n",
        "                \"error\": str(e),\n",
        "                \"traceback\": traceback.format_exc()\n",
        "            }\n",
        "\n",
        "    def format_result(self, result: Any) -> dict:\n",
        "        \"\"\"Format the result with metadata.\"\"\"\n",
        "        return {\n",
        "            \"tool_executed\": True,\n",
        "            \"result\": result,\n",
        "            \"timestamp\": time.strftime(\"%Y-%m-%dT%H:%M:%S%z\")\n",
        "        }\n",
        "\n",
        "\n",
        "class AgentLanguage:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def construct_prompt(self,\n",
        "                         actions: List[Action],\n",
        "                         environment: Environment,\n",
        "                         goals: List[Goal],\n",
        "                         memory: Memory) -> Prompt:\n",
        "        raise NotImplementedError(\"Subclasses must implement this method\")\n",
        "\n",
        "\n",
        "    def parse_response(self, response: str) -> dict:\n",
        "        raise NotImplementedError(\"Subclasses must implement this method\")\n",
        "\n",
        "\n",
        "\n",
        "class AgentFunctionCallingActionLanguage(AgentLanguage):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    # def format_goals(self, goals: List[Goal]) -> List:\n",
        "    #     # Map all goals to a single string that concatenates their description\n",
        "    #     # and combine into a single message of type system\n",
        "    #     sep = \"\\n-------------------\\n\"\n",
        "    #     goal_instructions = \"\\n\\n\".join([f\"{goal.name}:{sep}{goal.description}{sep}\" for goal in goals])\n",
        "    #     return [\n",
        "    #         {\"role\": \"system\", \"content\": goal_instructions}\n",
        "    #     ]\n",
        "\n",
        "    # def format_memory(self, memory: Memory) -> List:\n",
        "    #     \"\"\"Generate response from language model\"\"\"\n",
        "    #     # Map all environment results to a role:user messages\n",
        "    #     # Map all assistant messages to a role:assistant messages\n",
        "    #     # Map all user messages to a role:user messages\n",
        "    #     items = memory.get_memories()\n",
        "    #     mapped_items = []\n",
        "    #     for item in items:\n",
        "\n",
        "    #         content = item.get(\"content\", None)\n",
        "    #         if not content:\n",
        "    #             content = json.dumps(item, indent=4)\n",
        "\n",
        "    #         if item[\"type\"] == \"assistant\":\n",
        "    #             mapped_items.append({\"role\": \"assistant\", \"content\": content})\n",
        "    #         elif item[\"type\"] == \"environment\":\n",
        "    #             mapped_items.append({\"role\": \"assistant\", \"content\": content})\n",
        "    #         else:\n",
        "    #             mapped_items.append({\"role\": \"user\", \"content\": content})\n",
        "\n",
        "    #     return mapped_items\n",
        "\n",
        "\n",
        "\n",
        "    def format_goals(self, goals: List[Goal]) -> List:\n",
        "          sep = \"\\n-------------------\\n\"\n",
        "          goal_instructions = \"\\n\\n\".join([f\"{goal.name}:{sep}{goal.description}{sep}\" for goal in goals])\n",
        "\n",
        "          # Few-shot instructions + example tool calling sequence\n",
        "          few_shot_instructions = \"\"\"\n",
        "      You have the following tools available:\n",
        "\n",
        "      1. list_project_files() - no parameters, returns a list of Python files.\n",
        "      2. read_project_file(name: str) - reads the content of a specified file.\n",
        "      3. terminate(message: str) - writes the README file and ends the session.\n",
        "\n",
        "      Your task is to:\n",
        "\n",
        "      - First call list_project_files with an empty argument object: {\"tool\": \"list_project_files\", \"args\": {}}\n",
        "      - Then for each file in the result, call read_project_file with its filename, e.g.: {\"tool\": \"read_project_file\", \"args\": {\"name\": \"example.py\"}}\n",
        "      - After reading all files, call terminate with the README content, e.g.: {\"tool\": \"terminate\", \"args\": {\"message\": \"README content here\"}}\n",
        "\n",
        "      Respond **only** with a JSON-formatted function call as shown above. Do not write explanations or any other text.\n",
        "\n",
        "      Below is an example sequence of calls:\n",
        "\n",
        "      1. {\"tool\": \"list_project_files\", \"args\": {}}\n",
        "      2. {\"tool\": \"read_project_file\", \"args\": {\"name\": \"cl3-7.py\"}}\n",
        "      3. {\"tool\": \"read_project_file\", \"args\": {\"name\": \"cl3-clone.py\"}}\n",
        "      4. {\"tool\": \"read_project_file\", \"args\": {\"name\": \"deap_local.py\"}}\n",
        "      5. {\"tool\": \"terminate\", \"args\": {\"message\": \"Comprehensive README content summarizing the project.\"}}\n",
        "\n",
        "      Proceed step-by-step and do not skip any steps.\n",
        "      \"\"\"\n",
        "\n",
        "          full_instruction = goal_instructions + \"\\n\\n\" + few_shot_instructions\n",
        "\n",
        "          return [\n",
        "              {\"role\": \"system\", \"content\": full_instruction}\n",
        "          ]\n",
        "\n",
        "\n",
        "    def format_actions(self, actions: List[Action]) -> [List,List]:\n",
        "        \"\"\"Generate response from language model\"\"\"\n",
        "\n",
        "        tools = [\n",
        "            {\n",
        "                \"type\": \"function\",\n",
        "                \"function\": {\n",
        "                    \"name\": action.name,\n",
        "                    # Include up to 1024 characters of the description\n",
        "                    \"description\": action.description[:1024],\n",
        "                    \"parameters\": action.parameters,\n",
        "                },\n",
        "            } for action in actions\n",
        "        ]\n",
        "\n",
        "        return tools\n",
        "\n",
        "    def format_memory(self, memory: Memory) -> List:\n",
        "        items = memory.get_memories()\n",
        "        mapped_items = []\n",
        "        for item in items:\n",
        "            content = item.get(\"content\")\n",
        "            if not content:\n",
        "                content = json.dumps(item, indent=4)\n",
        "\n",
        "            if item[\"type\"] == \"environment\":\n",
        "                try:\n",
        "                    parsed = json.loads(content)\n",
        "                    pretty_content = json.dumps(parsed, indent=2)\n",
        "                    content = f\"Environment response:\\n{pretty_content}\"\n",
        "                except Exception:\n",
        "                    pass\n",
        "                mapped_items.append({\"role\": \"assistant\", \"content\": content})\n",
        "            elif item[\"type\"] == \"assistant\":\n",
        "                mapped_items.append({\"role\": \"assistant\", \"content\": content})\n",
        "            else:\n",
        "                mapped_items.append({\"role\": \"user\", \"content\": content})\n",
        "        return mapped_items\n",
        "\n",
        "\n",
        "    def construct_prompt(self,\n",
        "                         actions: List[Action],\n",
        "                         environment: Environment,\n",
        "                         goals: List[Goal],\n",
        "                         memory: Memory) -> Prompt:\n",
        "\n",
        "        prompt = []\n",
        "        prompt += self.format_goals(goals)\n",
        "        prompt += self.format_memory(memory)\n",
        "\n",
        "        tools = self.format_actions(actions)\n",
        "\n",
        "        return Prompt(messages=prompt, tools=tools)\n",
        "\n",
        "    def adapt_prompt_after_parsing_error(self,\n",
        "                                         prompt: Prompt,\n",
        "                                         response: str,\n",
        "                                         traceback: str,\n",
        "                                         error: Any,\n",
        "                                         retries_left: int) -> Prompt:\n",
        "\n",
        "        return prompt\n",
        "\n",
        "    # def parse_response(self, response: str) -> dict:\n",
        "    #     \"\"\"Parse LLM response into structured format by extracting the ```json block\"\"\"\n",
        "\n",
        "    #     try:\n",
        "    #         return json.loads(response)\n",
        "\n",
        "    #     except Exception as e:\n",
        "    #         return {\n",
        "    #             \"tool\": \"terminate\",\n",
        "    #             \"args\": {\"message\":response}\n",
        "    #         }\n",
        "\n",
        "    def parse_response(self, response: str) -> dict:\n",
        "        try:\n",
        "            data = json.loads(response)\n",
        "            if not isinstance(data, dict) or isinstance(data, list):\n",
        "                print(f\"[Warning] Unexpected response format: {data}\")\n",
        "                return {\"tool\": \"list_project_files\", \"args\": {}}\n",
        "            return data\n",
        "        except Exception as e:\n",
        "            print(f\"[Error] JSON parse failed: {e}\")\n",
        "            return {\"tool\": \"list_project_files\", \"args\": {}}\n",
        "\n",
        "\n",
        "\n",
        "class PythonActionRegistry(ActionRegistry):\n",
        "    def __init__(self, tags: List[str] = None, tool_names: List[str] = None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.terminate_tool = None\n",
        "\n",
        "        for tool_name, tool_desc in tools.items():\n",
        "            if tool_name == \"terminate\":\n",
        "                self.terminate_tool = tool_desc\n",
        "\n",
        "            if tool_names and tool_name not in tool_names:\n",
        "                continue\n",
        "\n",
        "            tool_tags = tool_desc.get(\"tags\", [])\n",
        "            if tags and not any(tag in tool_tags for tag in tags):\n",
        "                continue\n",
        "\n",
        "            self.register(Action(\n",
        "                name=tool_name,\n",
        "                function=tool_desc[\"function\"],\n",
        "                description=tool_desc[\"description\"],\n",
        "                parameters=tool_desc.get(\"parameters\", {}),\n",
        "                terminal=tool_desc.get(\"terminal\", False)\n",
        "            ))\n",
        "\n",
        "    def register_terminate_tool(self):\n",
        "        if self.terminate_tool:\n",
        "            self.register(Action(\n",
        "                name=\"terminate\",\n",
        "                function=self.terminate_tool[\"function\"],\n",
        "                description=self.terminate_tool[\"description\"],\n",
        "                parameters=self.terminate_tool.get(\"parameters\", {}),\n",
        "                terminal=self.terminate_tool.get(\"terminal\", False)\n",
        "            ))\n",
        "        else:\n",
        "            raise Exception(\"Terminate tool not found in tool registry\")\n",
        "\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self,\n",
        "                 goals: List[Goal],\n",
        "                 agent_language: AgentLanguage,\n",
        "                 action_registry: ActionRegistry,\n",
        "                 generate_response: Callable[[Prompt], str],\n",
        "                 environment: Environment):\n",
        "        \"\"\"\n",
        "        Initialize an agent with its core GAME components\n",
        "        \"\"\"\n",
        "        self.goals = goals\n",
        "        self.generate_response = generate_response\n",
        "        self.agent_language = agent_language\n",
        "        self.actions = action_registry\n",
        "        self.environment = environment\n",
        "\n",
        "    def construct_prompt(self, goals: List[Goal], memory: Memory, actions: ActionRegistry) -> Prompt:\n",
        "        \"\"\"Build prompt with memory context\"\"\"\n",
        "        return self.agent_language.construct_prompt(\n",
        "            actions=actions.get_actions(),\n",
        "            environment=self.environment,\n",
        "            goals=goals,\n",
        "            memory=memory\n",
        "        )\n",
        "\n",
        "    # def get_action(self, response):\n",
        "    #     invocation = self.agent_language.parse_response(response)\n",
        "    #     action = self.actions.get_action(invocation[\"tool\"])\n",
        "    #     return action, invocation\n",
        "\n",
        "    def get_action(self, response: str):\n",
        "        invocation = self.agent_language.parse_response(response)\n",
        "\n",
        "        if not isinstance(invocation, dict):\n",
        "            raise ValueError(f\"parse_response did not return a dict: got {invocation} (type: {type(invocation)})\")\n",
        "        if \"tool\" not in invocation:\n",
        "            raise ValueError(f\"'tool' key not found in invocation: {invocation}\")\n",
        "\n",
        "        action = self.actions.get_action(invocation[\"tool\"])\n",
        "        return action, invocation\n",
        "\n",
        "    def should_terminate(self, response: str) -> bool:\n",
        "        action_def, _ = self.get_action(response)\n",
        "        return action_def.terminal\n",
        "\n",
        "    def set_current_task(self, memory: Memory, task: str):\n",
        "        memory.add_memory({\"type\": \"user\", \"content\": task})\n",
        "\n",
        "    def update_memory(self, memory: Memory, response: str, result: dict):\n",
        "        \"\"\"\n",
        "        Update memory with the agent's decision and the environment's response.\n",
        "        \"\"\"\n",
        "        new_memories = [\n",
        "            {\"type\": \"assistant\", \"content\": response},\n",
        "            {\"type\": \"environment\", \"content\": json.dumps(result)}\n",
        "        ]\n",
        "        for m in new_memories:\n",
        "            memory.add_memory(m)\n",
        "\n",
        "    def prompt_llm_for_action(self, full_prompt: Prompt) -> str:\n",
        "        response = self.generate_response(full_prompt)\n",
        "        return response\n",
        "\n",
        "    def run(self, user_input: str, memory=None, max_iterations: int = 50) -> Memory:\n",
        "        \"\"\"\n",
        "        Execute the GAME loop for this agent with a maximum iteration limit.\n",
        "        \"\"\"\n",
        "        memory = memory or Memory()\n",
        "        self.set_current_task(memory, user_input)\n",
        "\n",
        "        for _ in range(max_iterations):\n",
        "            # Construct a prompt that includes the Goals, Actions, and the current Memory\n",
        "            prompt = self.construct_prompt(self.goals, memory, self.actions)\n",
        "\n",
        "            print(\"Agent thinking...\")\n",
        "            # Generate a response from the agent\n",
        "            response = self.prompt_llm_for_action(prompt)\n",
        "            print(f\"Agent Decision: {response}\")\n",
        "\n",
        "            # Determine which action the agent wants to execute\n",
        "            action, invocation = self.get_action(response)\n",
        "\n",
        "            # Execute the action in the environment\n",
        "            result = self.environment.execute_action(action, invocation[\"args\"])\n",
        "            print(f\"Action Result: {result}\")\n",
        "\n",
        "            # Update the agent's memory with information about what happened\n",
        "            self.update_memory(memory, response, result)\n",
        "\n",
        "            # Check if the agent has decided to terminate\n",
        "            if self.should_terminate(response):\n",
        "                break\n",
        "\n",
        "        return memory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6PC3ncxezoJC",
        "outputId": "288c1d92-a7be-45b6-e180-8fcf5f4cdb5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent thinking...\n",
            "=== Full API Response ===\n",
            "ModelResponse(id='chatcmpl-2e77eeff-0956-4fe9-b7d3-34faea60543b', created=1753476889, model='meta-llama/llama-4-scout-17b-16e-instruct', object='chat.completion', system_fingerprint='fp_37da608fc1', choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{}', name='list_project_files'), id='p257zdz2y', type='function')], function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=17, prompt_tokens=1263, total_tokens=1280, completion_tokens_details=None, prompt_tokens_details=None, queue_time=0.212414775, prompt_time=0.037416927, completion_time=0.043729362, total_time=0.081146289), usage_breakdown=None, x_groq={'id': 'req_01k11nvz3mekyr4b7mq2qy3xpk'}, service_tier='auto')\n",
            "=========================\n",
            "Raw model output:\n",
            "None\n",
            "--- End of output ---\n",
            "\n",
            "[Warning] Model returned empty response. Using fallback: {\"tool\": \"list_project_files\", \"args\": {}}\n",
            "Agent Decision: {\"tool\": \"list_project_files\", \"args\": {}}\n",
            "Action Result: {'tool_executed': True, 'result': ['cl3-7.py', 'cl3-clone.py', 'deap_local.py'], 'timestamp': '2025-07-25T20:54:50+0000'}\n",
            "Agent thinking...\n",
            "=== Full API Response ===\n",
            "ModelResponse(id='chatcmpl-28088147-20fa-4ffc-a228-c10384d1749f', created=1753476890, model='meta-llama/llama-4-scout-17b-16e-instruct', object='chat.completion', system_fingerprint='fp_79da0e0073', choices=[Choices(finish_reason='stop', index=0, message=Message(content='', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=1, prompt_tokens=1338, total_tokens=1339, completion_tokens_details=None, prompt_tokens_details=None, queue_time=-9223372036.894348, prompt_time=0.039572065, completion_time=0.002536444, total_time=0.042108509), usage_breakdown=None, x_groq={'id': 'req_01k11nvze3ehm9ysppwhr3fjjv'}, service_tier='auto')\n",
            "=========================\n",
            "Raw model output:\n",
            "\n",
            "--- End of output ---\n",
            "\n",
            "[Warning] Model returned empty response. Using fallback: {\"tool\": \"list_project_files\", \"args\": {}}\n",
            "Agent Decision: {\"tool\": \"list_project_files\", \"args\": {}}\n",
            "Action Result: {'tool_executed': True, 'result': ['cl3-7.py', 'cl3-clone.py', 'deap_local.py'], 'timestamp': '2025-07-25T20:54:50+0000'}\n",
            "Agent thinking...\n",
            "=== Full API Response ===\n",
            "ModelResponse(id='chatcmpl-8c3ecd40-da72-4c02-9ad2-5fa9400c5e27', created=1753476890, model='meta-llama/llama-4-scout-17b-16e-instruct', object='chat.completion', system_fingerprint='fp_37da608fc1', choices=[Choices(finish_reason='stop', index=0, message=Message(content='', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=1, prompt_tokens=1418, total_tokens=1419, completion_tokens_details=None, prompt_tokens_details=None, queue_time=-9223372036.90262, prompt_time=0.047844455, completion_time=0.002548757, total_time=0.050393212), usage_breakdown=None, x_groq={'id': 'req_01k11nvzqaekyvh0wmh8vpzb5q'}, service_tier='auto')\n",
            "=========================\n",
            "Raw model output:\n",
            "\n",
            "--- End of output ---\n",
            "\n",
            "[Warning] Model returned empty response. Using fallback: {\"tool\": \"list_project_files\", \"args\": {}}\n",
            "Agent Decision: {\"tool\": \"list_project_files\", \"args\": {}}\n",
            "Action Result: {'tool_executed': True, 'result': ['cl3-7.py', 'cl3-clone.py', 'deap_local.py'], 'timestamp': '2025-07-25T20:54:50+0000'}\n",
            "Agent thinking...\n",
            "=== Full API Response ===\n",
            "ModelResponse(id='chatcmpl-2006d473-8a5e-4296-a7ca-819951d7b8cf', created=1753476890, model='meta-llama/llama-4-scout-17b-16e-instruct', object='chat.completion', system_fingerprint='fp_79da0e0073', choices=[Choices(finish_reason='stop', index=0, message=Message(content='', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=1, prompt_tokens=1498, total_tokens=1499, completion_tokens_details=None, prompt_tokens_details=None, queue_time=-9223372036.897192, prompt_time=0.042414695, completion_time=0.002554004, total_time=0.044968699), usage_breakdown=None, x_groq={'id': 'req_01k11nw00men2vxqza7tzvt3gw'}, service_tier='auto')\n",
            "=========================\n",
            "Raw model output:\n",
            "\n",
            "--- End of output ---\n",
            "\n",
            "[Warning] Model returned empty response. Using fallback: {\"tool\": \"list_project_files\", \"args\": {}}\n",
            "Agent Decision: {\"tool\": \"list_project_files\", \"args\": {}}\n",
            "Action Result: {'tool_executed': True, 'result': ['cl3-7.py', 'cl3-clone.py', 'deap_local.py'], 'timestamp': '2025-07-25T20:54:50+0000'}\n",
            "Agent thinking...\n",
            "=== Full API Response ===\n",
            "ModelResponse(id='chatcmpl-719f45a2-b512-4916-900f-bc3bcebcfdaa', created=1753476891, model='meta-llama/llama-4-scout-17b-16e-instruct', object='chat.completion', system_fingerprint='fp_79da0e0073', choices=[Choices(finish_reason='stop', index=0, message=Message(content='', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=1, prompt_tokens=1578, total_tokens=1579, completion_tokens_details=None, prompt_tokens_details=None, queue_time=-9223372036.903145, prompt_time=0.048369123, completion_time=0.002538144, total_time=0.050907267), usage_breakdown=None, x_groq={'id': 'req_01k11nw09zfw1r7m4qb31frq1v'}, service_tier='auto')\n",
            "=========================\n",
            "Raw model output:\n",
            "\n",
            "--- End of output ---\n",
            "\n",
            "[Warning] Model returned empty response. Using fallback: {\"tool\": \"list_project_files\", \"args\": {}}\n",
            "Agent Decision: {\"tool\": \"list_project_files\", \"args\": {}}\n",
            "Action Result: {'tool_executed': True, 'result': ['cl3-7.py', 'cl3-clone.py', 'deap_local.py'], 'timestamp': '2025-07-25T20:54:51+0000'}\n",
            "Agent thinking...\n",
            "=== Full API Response ===\n",
            "ModelResponse(id='chatcmpl-b9847496-aed5-4a28-b3f5-83493bb1e01b', created=1753476891, model='meta-llama/llama-4-scout-17b-16e-instruct', object='chat.completion', system_fingerprint='fp_37da608fc1', choices=[Choices(finish_reason='stop', index=0, message=Message(content='', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=1, prompt_tokens=1658, total_tokens=1659, completion_tokens_details=None, prompt_tokens_details=None, queue_time=-9223372036.910109, prompt_time=0.055331969, completion_time=0.002537598, total_time=0.057869567), usage_breakdown=None, x_groq={'id': 'req_01k11nw0k9fwvvvaa18c2dg6t2'}, service_tier='auto')\n",
            "=========================\n",
            "Raw model output:\n",
            "\n",
            "--- End of output ---\n",
            "\n",
            "[Warning] Model returned empty response. Using fallback: {\"tool\": \"list_project_files\", \"args\": {}}\n",
            "Agent Decision: {\"tool\": \"list_project_files\", \"args\": {}}\n",
            "Action Result: {'tool_executed': True, 'result': ['cl3-7.py', 'cl3-clone.py', 'deap_local.py'], 'timestamp': '2025-07-25T20:54:51+0000'}\n",
            "Agent thinking...\n",
            "=== Full API Response ===\n",
            "ModelResponse(id='chatcmpl-8d710f46-922c-491f-9731-ba94bb9adb19', created=1753476892, model='meta-llama/llama-4-scout-17b-16e-instruct', object='chat.completion', system_fingerprint='fp_79da0e0073', choices=[Choices(finish_reason='stop', index=0, message=Message(content='', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=1, prompt_tokens=1738, total_tokens=1739, completion_tokens_details=None, prompt_tokens_details=None, queue_time=-9223372036.922773, prompt_time=0.067997727, completion_time=0.002544883, total_time=0.07054261), usage_breakdown=None, x_groq={'id': 'req_01k11nw14xe8xa3j95meah5f69'}, service_tier='auto')\n",
            "=========================\n",
            "Raw model output:\n",
            "\n",
            "--- End of output ---\n",
            "\n",
            "[Warning] Model returned empty response. Using fallback: {\"tool\": \"list_project_files\", \"args\": {}}\n",
            "Agent Decision: {\"tool\": \"list_project_files\", \"args\": {}}\n",
            "Action Result: {'tool_executed': True, 'result': ['cl3-7.py', 'cl3-clone.py', 'deap_local.py'], 'timestamp': '2025-07-25T20:54:52+0000'}\n",
            "Agent thinking...\n",
            "=== Full API Response ===\n",
            "ModelResponse(id='chatcmpl-f2f81c82-7403-4720-9c28-5d470833a99e', created=1753476892, model='meta-llama/llama-4-scout-17b-16e-instruct', object='chat.completion', system_fingerprint='fp_37da608fc1', choices=[Choices(finish_reason='stop', index=0, message=Message(content='', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=1, prompt_tokens=1818, total_tokens=1819, completion_tokens_details=None, prompt_tokens_details=None, queue_time=-9223372036.918844, prompt_time=0.064068621, completion_time=0.002540797, total_time=0.066609418), usage_breakdown=None, x_groq={'id': 'req_01k11nw1zrfjatj0qebek8yp3k'}, service_tier='auto')\n",
            "=========================\n",
            "Raw model output:\n",
            "\n",
            "--- End of output ---\n",
            "\n",
            "[Warning] Model returned empty response. Using fallback: {\"tool\": \"list_project_files\", \"args\": {}}\n",
            "Agent Decision: {\"tool\": \"list_project_files\", \"args\": {}}\n",
            "Action Result: {'tool_executed': True, 'result': ['cl3-7.py', 'cl3-clone.py', 'deap_local.py'], 'timestamp': '2025-07-25T20:54:52+0000'}\n",
            "Agent thinking...\n",
            "=== Full API Response ===\n",
            "ModelResponse(id='chatcmpl-579adb59-f1dc-4728-95ba-95484350f030', created=1753476893, model='meta-llama/llama-4-scout-17b-16e-instruct', object='chat.completion', system_fingerprint='fp_37da608fc1', choices=[Choices(finish_reason='stop', index=0, message=Message(content='', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=1, prompt_tokens=1898, total_tokens=1899, completion_tokens_details=None, prompt_tokens_details=None, queue_time=-9223372036.917665, prompt_time=0.062889071, completion_time=0.002539028, total_time=0.065428099), usage_breakdown=None, x_groq={'id': 'req_01k11nw29rekytg79da28bxrde'}, service_tier='auto')\n",
            "=========================\n",
            "Raw model output:\n",
            "\n",
            "--- End of output ---\n",
            "\n",
            "[Warning] Model returned empty response. Using fallback: {\"tool\": \"list_project_files\", \"args\": {}}\n",
            "Agent Decision: {\"tool\": \"list_project_files\", \"args\": {}}\n",
            "Action Result: {'tool_executed': True, 'result': ['cl3-7.py', 'cl3-clone.py', 'deap_local.py'], 'timestamp': '2025-07-25T20:54:53+0000'}\n",
            "Agent thinking...\n",
            "=== Full API Response ===\n",
            "ModelResponse(id='chatcmpl-618c31dc-76ea-446e-bcdf-c8b68aeb732c', created=1753476893, model='meta-llama/llama-4-scout-17b-16e-instruct', object='chat.completion', system_fingerprint='fp_37da608fc1', choices=[Choices(finish_reason='stop', index=0, message=Message(content='', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=1, prompt_tokens=1978, total_tokens=1979, completion_tokens_details=None, prompt_tokens_details=None, queue_time=-9223372036.926182, prompt_time=0.071406115, completion_time=0.002563827, total_time=0.073969942), usage_breakdown=None, x_groq={'id': 'req_01k11nw2khekysjagxsavth5p7'}, service_tier='auto')\n",
            "=========================\n",
            "Raw model output:\n",
            "\n",
            "--- End of output ---\n",
            "\n",
            "[Warning] Model returned empty response. Using fallback: {\"tool\": \"list_project_files\", \"args\": {}}\n",
            "Agent Decision: {\"tool\": \"list_project_files\", \"args\": {}}\n",
            "Action Result: {'tool_executed': True, 'result': ['cl3-7.py', 'cl3-clone.py', 'deap_local.py'], 'timestamp': '2025-07-25T20:54:53+0000'}\n",
            "Agent thinking...\n",
            "=== Full API Response ===\n",
            "ModelResponse(id='chatcmpl-1df5c8f6-667a-49e2-b249-56a40615d20e', created=1753476893, model='meta-llama/llama-4-scout-17b-16e-instruct', object='chat.completion', system_fingerprint='fp_37da608fc1', choices=[Choices(finish_reason='stop', index=0, message=Message(content='', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=1, prompt_tokens=2058, total_tokens=2059, completion_tokens_details=None, prompt_tokens_details=None, queue_time=-9223372036.921398, prompt_time=0.066621138, completion_time=0.002542077, total_time=0.069163215), usage_breakdown=None, x_groq={'id': 'req_01k11nw2xqfw290zmpkjcrrfrf'}, service_tier='auto')\n",
            "=========================\n",
            "Raw model output:\n",
            "\n",
            "--- End of output ---\n",
            "\n",
            "[Warning] Model returned empty response. Using fallback: {\"tool\": \"list_project_files\", \"args\": {}}\n",
            "Agent Decision: {\"tool\": \"list_project_files\", \"args\": {}}\n",
            "Action Result: {'tool_executed': True, 'result': ['cl3-7.py', 'cl3-clone.py', 'deap_local.py'], 'timestamp': '2025-07-25T20:54:53+0000'}\n",
            "Agent thinking...\n",
            "=== Full API Response ===\n",
            "ModelResponse(id='chatcmpl-1f6f6951-f1ef-44be-83f1-634e80c10316', created=1753476894, model='meta-llama/llama-4-scout-17b-16e-instruct', object='chat.completion', system_fingerprint='fp_37da608fc1', choices=[Choices(finish_reason='stop', index=0, message=Message(content='', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=1, prompt_tokens=2138, total_tokens=2139, completion_tokens_details=None, prompt_tokens_details=None, queue_time=-9223372036.918375, prompt_time=0.063598906, completion_time=0.002556358, total_time=0.066155264), usage_breakdown=None, x_groq={'id': 'req_01k11nw37pfw2ae35kjdtwrsth'}, service_tier='auto')\n",
            "=========================\n",
            "Raw model output:\n",
            "\n",
            "--- End of output ---\n",
            "\n",
            "[Warning] Model returned empty response. Using fallback: {\"tool\": \"list_project_files\", \"args\": {}}\n",
            "Agent Decision: {\"tool\": \"list_project_files\", \"args\": {}}\n",
            "Action Result: {'tool_executed': True, 'result': ['cl3-7.py', 'cl3-clone.py', 'deap_local.py'], 'timestamp': '2025-07-25T20:54:54+0000'}\n",
            "Agent thinking...\n",
            "=== Full API Response ===\n",
            "ModelResponse(id='chatcmpl-aec78624-3451-45e8-a801-6ddd358782f3', created=1753476894, model='meta-llama/llama-4-scout-17b-16e-instruct', object='chat.completion', system_fingerprint='fp_37da608fc1', choices=[Choices(finish_reason='stop', index=0, message=Message(content='', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=1, prompt_tokens=2218, total_tokens=2219, completion_tokens_details=None, prompt_tokens_details=None, queue_time=-9223372036.922117, prompt_time=0.067341302, completion_time=0.002539608, total_time=0.06988091), usage_breakdown=None, x_groq={'id': 'req_01k11nw3jpfdwsb9yb1yhhwky9'}, service_tier='auto')\n",
            "=========================\n",
            "Raw model output:\n",
            "\n",
            "--- End of output ---\n",
            "\n",
            "[Warning] Model returned empty response. Using fallback: {\"tool\": \"list_project_files\", \"args\": {}}\n",
            "Agent Decision: {\"tool\": \"list_project_files\", \"args\": {}}\n",
            "Action Result: {'tool_executed': True, 'result': ['cl3-7.py', 'cl3-clone.py', 'deap_local.py'], 'timestamp': '2025-07-25T20:54:54+0000'}\n",
            "Agent thinking...\n",
            "=== Full API Response ===\n",
            "ModelResponse(id='chatcmpl-ba7a0e6e-085d-47c7-a8d0-a93da4297a87', created=1753476894, model='meta-llama/llama-4-scout-17b-16e-instruct', object='chat.completion', system_fingerprint='fp_37da608fc1', choices=[Choices(finish_reason='stop', index=0, message=Message(content='', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=1, prompt_tokens=2298, total_tokens=2299, completion_tokens_details=None, prompt_tokens_details=None, queue_time=-9223372036.921299, prompt_time=0.066521829, completion_time=0.002564507, total_time=0.069086336), usage_breakdown=None, x_groq={'id': 'req_01k11nw3x8ekzay29dwc21txtn'}, service_tier='auto')\n",
            "=========================\n",
            "Raw model output:\n",
            "\n",
            "--- End of output ---\n",
            "\n",
            "[Warning] Model returned empty response. Using fallback: {\"tool\": \"list_project_files\", \"args\": {}}\n",
            "Agent Decision: {\"tool\": \"list_project_files\", \"args\": {}}\n",
            "Action Result: {'tool_executed': True, 'result': ['cl3-7.py', 'cl3-clone.py', 'deap_local.py'], 'timestamp': '2025-07-25T20:54:54+0000'}\n",
            "Agent thinking...\n",
            "=== Full API Response ===\n",
            "ModelResponse(id='chatcmpl-161bd3c2-98e5-4489-be42-9ee7af596de5', created=1753476895, model='meta-llama/llama-4-scout-17b-16e-instruct', object='chat.completion', system_fingerprint='fp_37da608fc1', choices=[Choices(finish_reason='stop', index=0, message=Message(content='', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=1, prompt_tokens=2378, total_tokens=2379, completion_tokens_details=None, prompt_tokens_details=None, queue_time=-9223372036.932207, prompt_time=0.077431582, completion_time=0.002539928, total_time=0.07997151), usage_breakdown=None, x_groq={'id': 'req_01k11nw47mekzazhbv60czmeft'}, service_tier='auto')\n",
            "=========================\n",
            "Raw model output:\n",
            "\n",
            "--- End of output ---\n",
            "\n",
            "[Warning] Model returned empty response. Using fallback: {\"tool\": \"list_project_files\", \"args\": {}}\n",
            "Agent Decision: {\"tool\": \"list_project_files\", \"args\": {}}\n",
            "Action Result: {'tool_executed': True, 'result': ['cl3-7.py', 'cl3-clone.py', 'deap_local.py'], 'timestamp': '2025-07-25T20:54:55+0000'}\n",
            "Agent thinking...\n",
            "=== Full API Response ===\n",
            "ModelResponse(id='chatcmpl-4e316732-c2a7-4d1f-bd5e-fbd32ddf06dc', created=1753476895, model='meta-llama/llama-4-scout-17b-16e-instruct', object='chat.completion', system_fingerprint='fp_79da0e0073', choices=[Choices(finish_reason='stop', index=0, message=Message(content='', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=1, prompt_tokens=2458, total_tokens=2459, completion_tokens_details=None, prompt_tokens_details=None, queue_time=-9223372036.925797, prompt_time=0.071020518, completion_time=0.002545483, total_time=0.073566001), usage_breakdown=None, x_groq={'id': 'req_01k11nw4pwekzac1qx490k42m6'}, service_tier='auto')\n",
            "=========================\n",
            "Raw model output:\n",
            "\n",
            "--- End of output ---\n",
            "\n",
            "[Warning] Model returned empty response. Using fallback: {\"tool\": \"list_project_files\", \"args\": {}}\n",
            "Agent Decision: {\"tool\": \"list_project_files\", \"args\": {}}\n",
            "Action Result: {'tool_executed': True, 'result': ['cl3-7.py', 'cl3-clone.py', 'deap_local.py'], 'timestamp': '2025-07-25T20:54:55+0000'}\n",
            "Agent thinking...\n",
            "=== Full API Response ===\n",
            "ModelResponse(id='chatcmpl-c95df40c-6e43-4f19-8151-0f010b54cc86', created=1753476895, model='meta-llama/llama-4-scout-17b-16e-instruct', object='chat.completion', system_fingerprint='fp_37da608fc1', choices=[Choices(finish_reason='stop', index=0, message=Message(content='', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=1, prompt_tokens=2538, total_tokens=2539, completion_tokens_details=None, prompt_tokens_details=None, queue_time=-9223372036.930498, prompt_time=0.075721767, completion_time=0.002538517, total_time=0.078260284), usage_breakdown=None, x_groq={'id': 'req_01k11nw514fwws0gtmwesfvpc6'}, service_tier='auto')\n",
            "=========================\n",
            "Raw model output:\n",
            "\n",
            "--- End of output ---\n",
            "\n",
            "[Warning] Model returned empty response. Using fallback: {\"tool\": \"list_project_files\", \"args\": {}}\n",
            "Agent Decision: {\"tool\": \"list_project_files\", \"args\": {}}\n",
            "Action Result: {'tool_executed': True, 'result': ['cl3-7.py', 'cl3-clone.py', 'deap_local.py'], 'timestamp': '2025-07-25T20:54:56+0000'}\n",
            "Agent thinking...\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n"
          ]
        },
        {
          "ename": "RateLimitError",
          "evalue": "litellm.RateLimitError: RateLimitError: GroqException - {\"error\":{\"message\":\"Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01jydvpy3be018597m66jqf198` service tier `on_demand` on tokens per minute (TPM): Limit 30000, Used 29258, Requested 1617. Please try again in 1.75s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing\",\"type\":\"tokens\",\"code\":\"rate_limit_exceeded\"}}\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\u001b[0m in \u001b[0;36m_make_common_sync_call\u001b[0;34m(self, sync_httpx_client, provider_config, api_base, headers, data, timeout, litellm_params, logging_obj, stream, signed_json_body)\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m                 response = sync_httpx_client.post(\n\u001b[0m\u001b[1;32m    175\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_base\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, url, data, json, params, headers, stream, timeout, files, content, logging_obj)\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"status_code\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, url, data, json, params, headers, stream, timeout, files, content, logging_obj)\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m             \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    828\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPStatusError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPStatusError\u001b[0m: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/litellm/main.py\u001b[0m in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[0m\n\u001b[1;32m   1823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1824\u001b[0;31m             response = base_llm_http_handler.completion(\n\u001b[0m\u001b[1;32m   1825\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\u001b[0m in \u001b[0;36mcompletion\u001b[0;34m(self, model, messages, api_base, custom_llm_provider, model_response, encoding, logging_obj, optional_params, timeout, litellm_params, acompletion, stream, fake_stream, api_key, headers, client, provider_config)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m         response = self._make_common_sync_call(\n\u001b[0m\u001b[1;32m    471\u001b[0m             \u001b[0msync_httpx_client\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msync_httpx_client\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\u001b[0m in \u001b[0;36m_make_common_sync_call\u001b[0;34m(self, sync_httpx_client, provider_config, api_base, headers, data, timeout, litellm_params, logging_obj, stream, signed_json_body)\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprovider_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprovider_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\u001b[0m in \u001b[0;36m_handle_error\u001b[0;34m(self, e, provider_config)\u001b[0m\n\u001b[1;32m   2408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2409\u001b[0;31m         raise provider_config.get_error_class(\n\u001b[0m\u001b[1;32m   2410\u001b[0m             \u001b[0merror_message\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOpenAIError\u001b[0m: {\"error\":{\"message\":\"Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01jydvpy3be018597m66jqf198` service tier `on_demand` on tokens per minute (TPM): Limit 30000, Used 29258, Requested 1617. Please try again in 1.75s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing\",\"type\":\"tokens\",\"code\":\"rate_limit_exceeded\"}}\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-35-2157352417.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;31m# Run the agent with user input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Your process: 1. Use list_project_files. 2. For each file, use read_project_file. 3. After all files, call terminate with a README based on the contents you've read.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m \u001b[0mfinal_memory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_memories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-34-2851942815.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, user_input, memory, max_iterations)\u001b[0m\n\u001b[1;32m    626\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Agent thinking...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m             \u001b[0;31m# Generate a response from the agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_llm_for_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Agent Decision: {response}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-34-2851942815.py\u001b[0m in \u001b[0;36mprompt_llm_for_action\u001b[0;34m(self, full_prompt)\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprompt_llm_for_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_prompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPrompt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_prompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-34-2851942815.py\u001b[0m in \u001b[0;36mgenerate_response\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m    168\u001b[0m         )\n\u001b[1;32m    169\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         response = completion(\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mbase_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/litellm/utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1307\u001b[0m                     \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback_exception\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m                 )  # DO NOT MAKE THREADED - router retry fallback relies on this!\n\u001b[0;32m-> 1309\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/litellm/utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                     \u001b[0mprint_verbose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error while checking max token limit: {str(e)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m             \u001b[0;31m# MODEL CALL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m             \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m             if _is_streaming_request(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/litellm/main.py\u001b[0m in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[0m\n\u001b[1;32m   3425\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3426\u001b[0m         \u001b[0;31m## Map to OpenAI Exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3427\u001b[0;31m         raise exception_type(\n\u001b[0m\u001b[1;32m   3428\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3429\u001b[0m             \u001b[0mcustom_llm_provider\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_llm_provider\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\u001b[0m in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[1;32m   2299\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexception_mapping_worked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2300\u001b[0m             \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"litellm_response_headers\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlitellm_response_headers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2301\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2302\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2303\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0merror_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlitellm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLITELLM_EXCEPTION_TYPES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\u001b[0m in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mExceptionCheckers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_error_str_rate_limit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     \u001b[0mexception_mapping_worked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m                     raise RateLimitError(\n\u001b[0m\u001b[1;32m    330\u001b[0m                         \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"RateLimitError: {exception_provider} - {message}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m                         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRateLimitError\u001b[0m: litellm.RateLimitError: RateLimitError: GroqException - {\"error\":{\"message\":\"Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01jydvpy3be018597m66jqf198` service tier `on_demand` on tokens per minute (TPM): Limit 30000, Used 29258, Requested 1617. Please try again in 1.75s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing\",\"type\":\"tokens\",\"code\":\"rate_limit_exceeded\"}}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "    from typing import Any, Dict, List, Optional\n",
        "    import os\n",
        "\n",
        "    class ActionContext:\n",
        "        \"\"\"\n",
        "        Simple context object passed to tools/functions to provide runtime metadata and state.\n",
        "\n",
        "        Attributes:\n",
        "            current_user_input (Optional[str]): Latest user input or task description.\n",
        "            memory (List[Dict[str, Any]]): List of past conversation or environment interactions.\n",
        "            goals (List[Any]): Current goals of the agent.\n",
        "            environment_vars (Dict[str, Any]): Environment or configuration variables.\n",
        "            user_role (Optional[str]): Current user's role or permissions.\n",
        "        \"\"\"\n",
        "\n",
        "        def __init__(\n",
        "            self,\n",
        "            current_user_input: Optional[str] = None,\n",
        "            memory: Optional[List[Dict[str, Any]]] = None,\n",
        "            goals: Optional[List[Any]] = None,\n",
        "            environment_vars: Optional[Dict[str, Any]] = None,\n",
        "            user_role: Optional[str] = None,\n",
        "        ):\n",
        "            self.current_user_input = current_user_input\n",
        "            self.memory = memory if memory is not None else []\n",
        "            self.goals = goals if goals is not None else []\n",
        "            self.environment_vars = environment_vars if environment_vars is not None else {}\n",
        "            self.user_role = user_role\n",
        "\n",
        "        def add_to_memory(self, item: Dict[str, Any]) -> None:\n",
        "            \"\"\"Add a memory item to the context's memory list.\"\"\"\n",
        "            self.memory.append(item)\n",
        "\n",
        "        def log(self, message: str) -> None:\n",
        "            \"\"\"Simple log method; in real use replace with proper logging.\"\"\"\n",
        "            print(f\"[ActionContext LOG]: {message}\")\n",
        "\n",
        "\n",
        "    # First, we'll define our tools using decorators\n",
        "    @register_tool(tags=[\"file_operations\", \"list\"])\n",
        "    def list_project_files() -> List[str]:\n",
        "        \"\"\"Lists all Python files in the current project directory.\n",
        "\n",
        "        Scans the current directory and returns a sorted list of all files\n",
        "        that end with '.py'.\n",
        "\n",
        "        Returns:\n",
        "            A sorted list of Python filenames\n",
        "        \"\"\"\n",
        "        return sorted([file for file in os.listdir(\".\")\n",
        "                      if file.endswith(\".py\")])\n",
        "\n",
        "    #@register_tool(tags=[\"file_operations\", \"read\"])\n",
        "    @register_tool(\n",
        "    tags=[\"file_operations\", \"read\"],\n",
        "    description=\"Read the content of a project file by filename.\",\n",
        "    parameters_override={\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"name\": {\"type\": \"string\", \"description\": \"The filename to read, including extension.\"}\n",
        "        },\n",
        "        \"required\": [\"name\"]\n",
        "    }\n",
        ")\n",
        "    def read_project_file(name: str) -> str:\n",
        "            \"\"\"Reads and returns the content of a specified project file.\n",
        "\n",
        "            Opens the file in read mode and returns its entire contents as a string.\n",
        "            Raises FileNotFoundError if the file doesn't exist.\n",
        "\n",
        "            Args:\n",
        "                name: The name of the file to read\n",
        "\n",
        "            Returns:\n",
        "                The contents of the file as a string\n",
        "            \"\"\"\n",
        "            with open(name, \"r\") as f:\n",
        "                return f.read()\n",
        "\n",
        "\n",
        "\n",
        "    # @register_tool(tags=[\"documentation\"])\n",
        "    # def generate_technical_documentation(action_context: ActionContext, code_or_feature: str) -> str:\n",
        "    #     \"\"\"Generate technical documentation by consulting a senior technical writer.\"\"\"\n",
        "    #     return prompt_expert(\n",
        "    #         action_context=action_context,\n",
        "    #         description_of_expert=\"\"\"\n",
        "    #         You are a senior technical writer with 15 years of experience in software documentation...\n",
        "    #         \"\"\",\n",
        "    #         prompt=f\"\"\"\n",
        "    #         Please create comprehensive technical documentation for the following code or feature:\n",
        "\n",
        "    #         {code_or_feature}\n",
        "\n",
        "    #         Your documentation should include:\n",
        "    #         1. A clear overview...\n",
        "    #         2. Detailed explanation...\n",
        "    #         3. Key interfaces...\n",
        "    #         ...\n",
        "    #         \"\"\"\n",
        "    #     )\n",
        "\n",
        "    # @register_tool(tags=[\"testing\"])\n",
        "    # def design_test_suite(action_context: ActionContext, feature_description: str) -> str:\n",
        "    #     \"\"\"Design a comprehensive test suite by consulting a senior QA engineer.\"\"\"\n",
        "    #     return prompt_expert(\n",
        "    #         action_context=action_context,\n",
        "    #         description_of_expert=\"\"\"\n",
        "    #         You are a senior QA engineer with 12 years of experience...\n",
        "    #         \"\"\",\n",
        "    #         prompt=f\"\"\"\n",
        "    #         Please design a comprehensive test suite for the following feature:\n",
        "\n",
        "    #         {feature_description}\n",
        "\n",
        "    #         Your test design should cover:\n",
        "    #         1. Unit tests...\n",
        "    #         2. Integration tests...\n",
        "    #         ...\n",
        "    #         \"\"\"\n",
        "    #     )\n",
        "\n",
        "    # @register_tool(tags=[\"code_quality\"])\n",
        "    # def perform_code_review(action_context: ActionContext, code: str) -> str:\n",
        "    #     \"\"\"Review code and suggest improvements by consulting a senior software architect.\"\"\"\n",
        "    #     return prompt_expert(\n",
        "    #         action_context=action_context,\n",
        "    #         description_of_expert=\"\"\"\n",
        "    #         You are a senior software architect with 20 years of experience...\n",
        "    #         \"\"\",\n",
        "    #         prompt=f\"\"\"\n",
        "    #         Please review the following code and provide detailed improvement suggestions:\n",
        "\n",
        "    #         {code}\n",
        "\n",
        "    #         Consider and address:\n",
        "    #         1. Code organization...\n",
        "    #         ...\n",
        "    #         \"\"\"\n",
        "    #     )\n",
        "\n",
        "    # @register_tool(tags=[\"communication\"])\n",
        "    # def write_feature_announcement(action_context: ActionContext, feature_details: str, audience: str) -> str:\n",
        "    #     \"\"\"Write a feature announcement by consulting a product marketing expert.\"\"\"\n",
        "    #     return prompt_expert(\n",
        "    #         action_context=action_context,\n",
        "    #         description_of_expert=\"\"\"\n",
        "    #         You are a senior product marketing manager with 12 years of experience...\n",
        "    #         \"\"\",\n",
        "    #         prompt=f\"\"\"\n",
        "    #         Please write a feature announcement for the following feature:\n",
        "\n",
        "    #         {feature_details}\n",
        "\n",
        "    #         This announcement is intended for a {audience} audience.\n",
        "\n",
        "    #         Your announcement should include:\n",
        "    #         1. A compelling introduction...\n",
        "    #         ...\n",
        "    #         \"\"\"\n",
        "    #     )\n",
        "\n",
        "    # @register_tool(tags=[\"system\"], terminal=True)\n",
        "    # def terminate(readme_content: str) -> str:\n",
        "    #     \"\"\"Writes a README.md file and terminates the agent.\"\"\"\n",
        "    #     with open(\"README.md\", \"w\") as f:\n",
        "    #         f.write(readme_content)\n",
        "    #     return f\"README.md successfully written with content:\\n\\n{readme_content}\"\n",
        "\n",
        "\n",
        "\n",
        "    @register_tool(tags=[\"documentation\"])\n",
        "    def generate_technical_documentation(action_context: ActionContext, code_or_feature: str) -> str:\n",
        "        \"\"\"\n",
        "        Generate technical documentation by consulting a senior technical writer.\n",
        "        This expert focuses on creating clear, comprehensive documentation for developers.\n",
        "\n",
        "        Args:\n",
        "            code_or_feature: The code or feature to document\n",
        "        \"\"\"\n",
        "        return prompt_expert(\n",
        "            action_context=action_context,\n",
        "            description_of_expert=\"\"\"\n",
        "            You are a senior technical writer with 15 years of experience in software documentation.\n",
        "            You have particular expertise in:\n",
        "            - Writing clear and precise API documentation\n",
        "            - Explaining complex technical concepts to developers\n",
        "            - Documenting implementation details and integration points\n",
        "            - Creating code examples that illustrate key concepts\n",
        "            - Identifying and documenting important caveats and edge cases\n",
        "\n",
        "            Your documentation is known for striking the perfect balance between completeness\n",
        "            and clarity. You understand that good technical documentation serves as both\n",
        "            a reference and a learning tool.\n",
        "            \"\"\",\n",
        "            prompt=f\"\"\"\n",
        "            Please create comprehensive technical documentation for the following code or feature:\n",
        "\n",
        "            {code_or_feature}\n",
        "\n",
        "            Your documentation should include:\n",
        "            1. A clear overview of the feature's purpose and functionality\n",
        "            2. Detailed explanation of the implementation approach\n",
        "            3. Key interfaces and integration points\n",
        "            4. Usage examples with code snippets\n",
        "            5. Important considerations and edge cases\n",
        "            6. Performance implications if relevant\n",
        "\n",
        "            Focus on providing information that developers need to effectively understand\n",
        "            and work with this code.\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "    @register_tool(tags=[\"testing\"])\n",
        "    def design_test_suite(action_context: ActionContext, feature_description: str) -> str:\n",
        "        \"\"\"\n",
        "        Design a comprehensive test suite by consulting a senior QA engineer.\n",
        "        This expert focuses on creating thorough test coverage with attention to edge cases.\n",
        "\n",
        "        Args:\n",
        "            feature_description: Description of the feature to test\n",
        "        \"\"\"\n",
        "        return prompt_expert(\n",
        "            action_context=action_context,\n",
        "            description_of_expert=\"\"\"\n",
        "            You are a senior QA engineer with 12 years of experience in test design and automation.\n",
        "            Your expertise includes:\n",
        "            - Comprehensive test strategy development\n",
        "            - Unit, integration, and end-to-end testing\n",
        "            - Performance and stress testing\n",
        "            - Security testing considerations\n",
        "            - Test automation best practices\n",
        "\n",
        "            You are particularly skilled at identifying edge cases and potential failure modes\n",
        "            that others might miss. Your test suites are known for their thoroughness and\n",
        "            their ability to catch issues early in the development cycle.\n",
        "            \"\"\",\n",
        "            prompt=f\"\"\"\n",
        "            Please design a comprehensive test suite for the following feature:\n",
        "\n",
        "            {feature_description}\n",
        "\n",
        "            Your test design should cover:\n",
        "            1. Unit tests for individual components\n",
        "            2. Integration tests for component interactions\n",
        "            3. End-to-end tests for critical user paths\n",
        "            4. Performance test scenarios if relevant\n",
        "            5. Edge cases and error conditions\n",
        "            6. Test data requirements\n",
        "\n",
        "            For each test category, provide:\n",
        "            - Specific test scenarios\n",
        "            - Expected outcomes\n",
        "            - Important edge cases to consider\n",
        "            - Potential testing challenges\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "    @register_tool(tags=[\"code_quality\"])\n",
        "    def perform_code_review(action_context: ActionContext, code: str) -> str:\n",
        "        \"\"\"\n",
        "        Review code and suggest improvements by consulting a senior software architect.\n",
        "        This expert focuses on code quality, architecture, and best practices.\n",
        "\n",
        "        Args:\n",
        "            code: The code to review\n",
        "        \"\"\"\n",
        "        return prompt_expert(\n",
        "            action_context=action_context,\n",
        "            description_of_expert=\"\"\"\n",
        "            You are a senior software architect with 20 years of experience in code review\n",
        "            and software design. Your expertise includes:\n",
        "            - Software architecture and design patterns\n",
        "            - Code quality and maintainability\n",
        "            - Performance optimization\n",
        "            - Scalability considerations\n",
        "            - Security best practices\n",
        "\n",
        "            You have a talent for identifying subtle design issues and suggesting practical\n",
        "            improvements that enhance code quality without over-engineering.\n",
        "            \"\"\",\n",
        "            prompt=f\"\"\"\n",
        "            Please review the following code and provide detailed improvement suggestions:\n",
        "\n",
        "            {code}\n",
        "\n",
        "            Consider and address:\n",
        "            1. Code organization and structure\n",
        "            2. Potential design pattern applications\n",
        "            3. Performance optimization opportunities\n",
        "            4. Error handling completeness\n",
        "            5. Edge case handling\n",
        "            6. Maintainability concerns\n",
        "\n",
        "            For each suggestion:\n",
        "            - Explain the current issue\n",
        "            - Provide the rationale for change\n",
        "            - Suggest specific improvements\n",
        "            - Note any trade-offs to consider\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "    @register_tool(tags=[\"communication\"])\n",
        "    def write_feature_announcement(action_context: ActionContext,\n",
        "                                feature_details: str,\n",
        "                                audience: str) -> str:\n",
        "        \"\"\"\n",
        "        Write a feature announcement by consulting a product marketing expert.\n",
        "        This expert focuses on clear communication of technical features to different audiences.\n",
        "\n",
        "        Args:\n",
        "            feature_details: Technical details of the feature\n",
        "            audience: Target audience for the announcement (e.g., \"technical\", \"business\")\n",
        "        \"\"\"\n",
        "        return prompt_expert(\n",
        "            action_context=action_context,\n",
        "            description_of_expert=\"\"\"\n",
        "            You are a senior product marketing manager with 12 years of experience in\n",
        "            technical product communication. Your expertise includes:\n",
        "            - Translating technical features into clear value propositions\n",
        "            - Crafting compelling product narratives\n",
        "            - Adapting messaging for different audience types\n",
        "            - Building excitement while maintaining accuracy\n",
        "            - Creating clear calls to action\n",
        "\n",
        "            You excel at finding the perfect balance between technical accuracy and\n",
        "            accessibility, ensuring your communications are both precise and engaging.\n",
        "            \"\"\",\n",
        "            prompt=f\"\"\"\n",
        "            Please write a feature announcement for the following feature:\n",
        "\n",
        "            {feature_details}\n",
        "\n",
        "            This announcement is intended for a {audience} audience.\n",
        "\n",
        "            Your announcement should include:\n",
        "            1. A compelling introduction\n",
        "            2. Clear explanation of the feature\n",
        "            3. Key benefits and use cases\n",
        "            4. Technical details (adapted to audience)\n",
        "            5. Implementation requirements\n",
        "            6. Next steps or call to action\n",
        "\n",
        "            Ensure the tone and technical depth are appropriate for a {audience} audience.\n",
        "            Focus on conveying both the value and the practical implications of this feature.\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "    # @register_tool(tags=[\"system\"], terminal=True)\n",
        "    # def terminate(readme_content: str) -> str:\n",
        "    #         \"\"\"Writes a README.md file and terminates the agent.\"\"\"\n",
        "    #         with open(\"README (4).md\", \"w\") as f:\n",
        "    #             f.write(readme_content)\n",
        "    #         return f\"README.md successfully written with content:\\n\\n{readme_content}\"\n",
        "\n",
        "    @register_tool(tags=[\"system\"], terminal=True)\n",
        "    def terminate(message: str) -> str:\n",
        "        with open(\"README.md\", \"w\") as f:\n",
        "            f.write(message)\n",
        "        return f\"README.md successfully written with content:\\n\\n{message}\"\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "    @register_tool(tags=[\"system\"], terminal=True)\n",
        "    def terminate(message: str) -> str:\n",
        "        # Terminates the agent's execution with a final message.\n",
        "\n",
        "        # Args:\n",
        "        #     message: The final message to return before terminating\n",
        "\n",
        "        # Returns:\n",
        "        #     The message with a termination note appended\n",
        "        \"\"\"\n",
        "        # return f\"{message}\\nTerminating...\"\n",
        "\n",
        "\n",
        "# Define the agent's goals\n",
        "goals = [\n",
        "    Goal(\n",
        "        priority=1,\n",
        "        name=\"List Project Files\",\n",
        "        description=(\n",
        "            \"Use the 'list_project_files' tool to get a list of all Python files in the project directory.\"\n",
        "            \" This will allow you to systematically process each file.\"\n",
        "        )\n",
        "    ),\n",
        "    Goal(\n",
        "        priority=1,\n",
        "        name=\"Read Project Files\",\n",
        "        description=(\n",
        "            \"For each file obtained from the file listing, use the 'read_project_file' tool to read its contents.\"\n",
        "            \" Store and integrate this information as part of your knowledge about the project.\"\n",
        "        )\n",
        "    ),\n",
        "    Goal(\n",
        "        priority=1,\n",
        "        name=\"Generate README\",\n",
        "        description=(\n",
        "            \"After reading all files, generate comprehensive technical documentation summarizing the\"\n",
        "            \" project’s functionality, key features, and usage.\"\n",
        "        )\n",
        "    ),\n",
        "    Goal(\n",
        "        priority=1,\n",
        "        name=\"Terminate\",\n",
        "        description=(\n",
        "            \"Call the 'terminate' tool when you have prepared the complete README content.\"\n",
        "        )\n",
        "    ),\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "    # Create an agent instance with tag-filtered actions\n",
        "agent = Agent(\n",
        "        goals=goals,\n",
        "        agent_language=AgentFunctionCallingActionLanguage(),\n",
        "        # The ActionRegistry now automatically loads tools with these tags\n",
        "        action_registry=PythonActionRegistry(tags=[\"file_operations\", \"read\",\"system\"]),\n",
        "        generate_response=generate_response,\n",
        "        environment=Environment()\n",
        "    )\n",
        "\n",
        "    # Run the agent with user input\n",
        "user_input = \"Your process: 1. Use list_project_files. 2. For each file, use read_project_file. 3. After all files, call terminate with a README based on the contents you've read.\"\n",
        "final_memory = agent.run(user_input, max_iterations=100)\n",
        "print(final_memory.get_memories())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
